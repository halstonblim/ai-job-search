{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d768df1a-3b7b-45d0-8b8c-ecce3df39d7e",
   "metadata": {},
   "source": [
    "# Job Search & Ranking Notebook\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Query the Google Custom Search JSON API for Machine Learning Engineer and Data Scientist job postings.  \n",
    "2. Extract and normalize key fields (title, company, location, description, unique ID).  \n",
    "3. Generalize the pipeline to return and process the top _N_ results.  \n",
    "4. Parse, clean, and rank the retrieved job listings for downstream analysis or matching.\n",
    "\n",
    "Eventually this will be integrated into an orchestrated agent workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaae23b0-7eb7-4882-b2e9-ea45559a46fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q openai-agents python-dotenv aiohttp backoff\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcdb6fd-d40e-402c-af0f-e56c22e2d62a",
   "metadata": {},
   "source": [
    "## Google Custom Search Engine\n",
    "A custom google search is a relatively inexpensive way to gather large number of job posting URLs. \n",
    "\n",
    "With a google search, you can also filter on domain (greenhouse.io) and other keywords (\"remote\", \"Machine Learning Engineer\", \"Data Scientist\")\n",
    "\n",
    "In contrast, integrated web tools called by LLMs typically only return a small number of results, with limited pagination functionality. Also pricey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1487e0f8-0084-46fc-937c-95eb6dd2061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, re, requests\n",
    "from itertools import count\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_job_id(url: str) -> str:\n",
    "    \"\"\"Greenhouse numeric ID if present, otherwise SHA-256 of the URL.\"\"\"\n",
    "    m = re.search(r\"/jobs/(\\d+)\", url)\n",
    "    return m.group(1) if m else hashlib.sha256(url.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def parse_google_jobs(json_data: dict) -> List[Dict]:\n",
    "    \"\"\"Transform one Google CSE response page into a list of normalised dicts.\"\"\"\n",
    "    jobs = []\n",
    "    for item in json_data.get(\"items\", []):\n",
    "        url = item[\"link\"]\n",
    "        jobs.append(\n",
    "            {\n",
    "                \"unique_id\": extract_job_id(url),\n",
    "                \"job_title\": (\n",
    "                    item.get(\"pagemap\", {})\n",
    "                        .get(\"metatags\", [{}])[0]\n",
    "                        .get(\"og:title\", item[\"title\"])\n",
    "                ),\n",
    "                \"url\": url,\n",
    "                \"company\": url.split(\"/\")[3] if \"://\" in url else None,\n",
    "            }\n",
    "        )\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def fetch_google_jobs(\n",
    "    *,\n",
    "    query: str,\n",
    "    api_key: str,\n",
    "    cse_id: str,\n",
    "    top_n: int = 50,\n",
    "    user_agent: str | None = None,\n",
    "    page_size: int = 10,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Return **up to** `top_n` unique Greenhouse jobs for `query`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_n     : how many results you *want* (any positive int, even >100).\n",
    "    page_size : 1-10.  Google will silently coerce values >10 down to 10.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Google never returns more than 100 total results per query and will\n",
    "      raise `400 INVALID_REQUEST` if  `start + num > 100`.  :contentReference[oaicite:0]{index=0}  \n",
    "    Each API call counts against your daily quota and costs $5 / 1 000 beyond\n",
    "      the first 100 free queries.  :contentReference[oaicite:1]{index=1}\n",
    "    \"\"\"\n",
    "    if top_n < 1:\n",
    "        return []\n",
    "\n",
    "    headers = {\"User-Agent\": user_agent} if user_agent else None\n",
    "    results, seen = [], set()\n",
    "\n",
    "    for start in count(1, page_size):\n",
    "        #  stop before Google returns 400 error.\n",
    "        if start > 99 or len(results) >= top_n:\n",
    "            break\n",
    "\n",
    "        resp = requests.get(\n",
    "            \"https://www.googleapis.com/customsearch/v1\",\n",
    "            params={\n",
    "                \"key\": api_key,\n",
    "                \"cx\": cse_id,\n",
    "                \"q\": query,\n",
    "                \"num\": min(page_size, 10),   # 10 is the API max\n",
    "                \"start\": start,\n",
    "            },\n",
    "            headers=headers,\n",
    "            timeout=30,\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        page_jobs = parse_google_jobs(resp.json())\n",
    "\n",
    "        if not page_jobs:             # no more hits – bail early\n",
    "            break\n",
    "\n",
    "        for job in page_jobs:\n",
    "            if job[\"unique_id\"] not in seen:\n",
    "                seen.add(job[\"unique_id\"])\n",
    "                results.append(job)\n",
    "                if len(results) == top_n:\n",
    "                    break\n",
    "\n",
    "    if len(results) < top_n:\n",
    "        print(\n",
    "            f\"[fetch_google_jobs] Requested {top_n} results but the API only \"\n",
    "            f\"returned {len(results)} (Google caps each query at 100).\"\n",
    "        )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21e892bd-19d3-4c74-994e-c764b3e4589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HARD_LIMIT = 99\n",
    "query = 'site:boards.greenhouse.io intext:\"Apply\" (intext:\"Machine Learning\" OR intext:\"Data Scientist\") \"remote\"'\n",
    "args = {\n",
    "    \"api_key\": config[\"GOOGLE_API_KEY\"],\n",
    "    \"cse_id\":  config[\"GOOGLE_CSE_ID\"],\n",
    "    \"query\":   query,\n",
    "    \"top_n\": 99,\n",
    "    \"page_size\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d8af4c-3f0f-46f0-9d5d-53bb34ca023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch_google_jobs] Requested 99 results but the API only returned 94 (Google caps each query at 100).\n"
     ]
    }
   ],
   "source": [
    "google_results = fetch_google_jobs(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b88324-4eb7-4d19-ade1-a82e4e938fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'unique_id': '6828507',\n",
       "  'job_title': 'Staff Data Scientist - Remote, United States',\n",
       "  'url': 'https://boards.greenhouse.io/toast/jobs/6828507',\n",
       "  'company': 'toast'},\n",
       " {'unique_id': '6799092',\n",
       "  'job_title': 'Stitch Fix, Your Personal Stylist',\n",
       "  'url': 'https://boards.greenhouse.io/stitchfix/jobs/6799092',\n",
       "  'company': 'stitchfix'},\n",
       " {'unique_id': '6751486',\n",
       "  'job_title': 'Staff Machine Learning Engineer, Edge AI/Model Optimization - Remote - US',\n",
       "  'url': 'https://boards.greenhouse.io/samsara/jobs/6751486',\n",
       "  'company': 'samsara'},\n",
       " {'unique_id': '5492159004',\n",
       "  'job_title': 'Senior Data Scientist, ePROs',\n",
       "  'url': 'https://boards.greenhouse.io/thymecare/jobs/5492159004',\n",
       "  'company': 'thymecare'},\n",
       " {'unique_id': '4919145',\n",
       "  'job_title': 'Work at Samsara: Apply to open roles today',\n",
       "  'url': 'https://boards.greenhouse.io/samsara/jobs/4919145',\n",
       "  'company': 'samsara'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20933c6a-ef29-417d-8283-e58589e6e75a",
   "metadata": {},
   "source": [
    "## ATS JSON Endpoint\n",
    "Many big ATSs (Applicant Tracking Service) publish an unauthenticated JSON endpoint that contains every field you care about—title, location, description, employment-type, compensation, etc. so you can skip any paid extraction service entirely\n",
    "\n",
    "Why use `aiohttp`? Mainly because we want to hit lots of URLs quickly from Python without spinning up threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b62c74-1696-4605-b308-9a7e79f3fcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, asyncio, aiohttp, json, time, backoff\n",
    "import html\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "GH_RE = re.compile(r'boards\\.greenhouse\\.io/(?P<co>[^/]+)/jobs/(?P<id>\\d+)')\n",
    "\n",
    "def api_from_job_url(url: str) -> str | None:\n",
    "    m = GH_RE.search(url)\n",
    "    if not m:            # not a Greenhouse link\n",
    "        return None\n",
    "    return f\"https://boards-api.greenhouse.io/v1/boards/{m.group('co')}/jobs/{m.group('id')}\"\n",
    "\n",
    "def html_to_plain(raw: str) -> str:\n",
    "    \"\"\"Greenhouse `content` → readable text.\"\"\"\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(html.unescape(raw), \"html.parser\")   # unescape *before* parse\n",
    "    for t in soup([\"script\", \"style\"]):\n",
    "        t.decompose()\n",
    "    text = soup.get_text(separator=\"\\n\\n\", strip=True)\n",
    "    text = html.unescape(text).replace(\"\\xa0\", \" \")\n",
    "    return \"\\n\".join(ln.rstrip() for ln in text.splitlines() if ln.strip())\n",
    "\n",
    "@backoff.on_exception(backoff.expo, aiohttp.ClientError, max_tries=5)\n",
    "async def fetch(session, job_url):\n",
    "    api = api_from_job_url(job_url)\n",
    "    if not api:\n",
    "        return {\"url\": job_url, \"status\": \"skip\"}\n",
    "    async with session.get(api, timeout=30) as r:\n",
    "        data = await r.json()\n",
    "        return {\n",
    "            \"url\": job_url,\n",
    "            \"status\": r.status,\n",
    "            \"company\": data.get(\"board_token\"),\n",
    "            \"job_id\": data.get(\"id\") if  data.get(\"id\") else \"\",\n",
    "            \"title\": data.get(\"title\") if data.get(\"title\") else \"\",\n",
    "            \"location\": data.get(\"location\", {}).get(\"name\") if data.get(\"location\", {}).get(\"name\") else \"\",\n",
    "            \"description\": html_to_plain(data.get(\"content\"),)\n",
    "        }\n",
    "\n",
    "async def grab(urls, concurrency=25):\n",
    "    async with aiohttp.ClientSession() as sess:\n",
    "        sem = asyncio.Semaphore(concurrency)\n",
    "        async def limited(u):\n",
    "            async with sem:\n",
    "                return await fetch(sess, u)\n",
    "        return await asyncio.gather(*(limited(u) for u in urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b478c4-e3d0-46a1-ae1c-93736116c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = await grab(job['url'] for job in google_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71df3dda-4526-4e90-b93b-73c130667226",
   "metadata": {},
   "source": [
    "The reason why we don't get all successes is because many google results only go to company job boards, not job posting themselves.\n",
    "- The website of the actual job posting requires going 1 level deeper than the actual job posting\n",
    "\n",
    "For now, we will omit these. But soon we wil set up an agent tool call which does this for us, e.g. https://chatgpt.com/share/6837b52e-8810-8012-9233-eb07c48e6510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85153366-7197-497f-adc7-09365397997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49 valid job descriptions out of 94 google results\n"
     ]
    }
   ],
   "source": [
    "success = lambda x: (x['status'] == 200)\n",
    "filtered_result = list(filter(valid_location, job_descriptions))\n",
    "print(f\"Found {len(filtered_result)} valid job descriptions out of {len(google_results)} google results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96489b25-7189-43aa-ab22-3a1855b780dd",
   "metadata": {},
   "source": [
    "## LLM Job Ranker\n",
    "\n",
    "For this kind of task, I found that o4-mini works really well, and gpt-4.1 also works fairly well, but the smaller non-reasoning models do not give reliable ranks that I would agree with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8b43758-0586-4348-acc1-88424a890b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\\\n",
    "You are a career assistant that helps the user determine whether a job is relevant or not. Given the job description, ranks the job based on how well the job description matches the user preferences, experience, and resume.\n",
    "\n",
    "For each job description provided output the following information\n",
    "1. Company name\n",
    "2. Job title\n",
    "3. Overall job match score which ranges from 1 (bad match) to 5 (great match) \n",
    "4. One sentence justification\n",
    "\n",
    "User preferences:\n",
    "- Job does not require significant software engineering background\n",
    "- Years of work experience less than 7\n",
    "\n",
    "User resume:\n",
    "{''.join(open('resume.txt','r').readlines())}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3420d830-57a7-4c36-9c2a-ac6fa72f3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = AsyncOpenAI()\n",
    "\n",
    "class JobRank(BaseModel):\n",
    "    company: str\n",
    "    job_title: str\n",
    "    rank: int\n",
    "    reason: str\n",
    "\n",
    "async def rank_job(job):\n",
    "    resp = await client.responses.parse(\n",
    "        model=\"o4-mini\",\n",
    "        input=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": f\"Job description:\\n{job['description']}\"},\n",
    "        ],\n",
    "        text_format=JobRank,\n",
    "    )\n",
    "    return resp.output_parsed\n",
    "\n",
    "async def main():\n",
    "    # schedule one task per job\n",
    "    tasks = [asyncio.create_task(rank_job(job)) for job in filtered_result]\n",
    "    # run them all concurrently\n",
    "    ranked = await asyncio.gather(*tasks)\n",
    "    return ranked\n",
    "\n",
    "final_result = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc4f6610-0a04-4675-b6a6-82bb3b6a5cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "552ab5bf-551d-4877-914a-0c278f8cbc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JobRank(company='Coinbase', job_title='Data Scientist', rank=4, reason='Your strong quantitative, Python and SQL experience and expertise in experimentation and modeling align well with this role’s requirements, though it involves more product-focused ETL and code review responsibilities than purely research work.')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
